{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        # Attention weights of the different heads are stored in the same tensors.\n",
    "\n",
    "        # We put all the attention heads in the same Linears Layers:\n",
    "        self.values = nn.Linear(self.embed_size, self.embed_size, bias=False)\n",
    "        self.keys = nn.Linear(self.embed_size, self.embed_size, bias=False)\n",
    "        self.queries = nn.Linear(self.embed_size, self.embed_size, bias=False)\n",
    "\n",
    "        # fc_out after having concatenated the results of the attention of each head\n",
    "        self.fc_out = nn.Linear(self.embed_size, embed_size)\n",
    "\n",
    "    def init2(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = self.embed_size//self.heads\n",
    "        assert (self.head_dim * self.heads == self.embed_size )\n",
    "        \n",
    "        self.values = nn.Linear(self.embed_size, self.embed_size, bias = False)\n",
    "        self.queries = nn.Linear(self.embed_size, self.embed_size, bias = False)\n",
    "        self.keys = nn.Linear(self.embed_size, self.embed_size, bias = False)\n",
    "\n",
    "        self.fc_out = nn.Linear(self.embed_size, self.embed_size)\n",
    "\n",
    "    def forward(self, values, keys, queries, mask):\n",
    "        \"\"\"\n",
    "        values: (N, value_len, embed_size)\n",
    "        keys: (N, keys_len, embed_size)\n",
    "        queries: (N, queries_len, embed_size)\n",
    "\n",
    "        mask: None or (N, heads, query_len, key_len) \n",
    "        if mask == 0, attention matrix  -> float(\"-1e20\") (big negative value)\n",
    "        Ignore the mask, it's too difficult at the beginning.\n",
    "        \"\"\"\n",
    "        # Get number of training examples\n",
    "        N = queries.shape[0]\n",
    "\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "        # Compute the values keys and queries\n",
    "        values = self.values(values)  # (N, value_len, embed_size)\n",
    "        keys = self.keys(keys)  # (N, key_len, embed_size)\n",
    "        queries = self.queries(queries)  # (N, query_len, embed_size)\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        # Compute the similarity between query*keys\n",
    "        # Einsum does matrix mult. for query*keys for each training example\n",
    "        # with every other training example, don't be confused by einsum\n",
    "        # it's just how I like doing matrix multiplication & bmm\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # queries shape: (N, query_len, heads, heads_dim),\n",
    "        # keys shape: (N, key_len, heads, heads_dim)\n",
    "        # energy: (N, heads, query_len, key_len)\n",
    "\n",
    "        # Mask padded indices so their weights become 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Normalize energy values similarly to seq2seq + attention\n",
    "        # so that they sum to 1. Also divide by scaling factor for better stability\n",
    "        attention = torch.softmax(energy / (self.head_dim ** (1 / 2)), dim=3)\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "\n",
    "\n",
    "        # We compute the attention (aka the ponderated value)\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "        # values shape: (N, value_len, heads, heads_dim)\n",
    "        # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
    "        # we reshape and flatten the last two dimensions.\n",
    "\n",
    "\n",
    "        # Linear layer doesn't modify the shape, final shape will be\n",
    "        # (N, query_len, embed_size)\n",
    "        return self.fc_out(out)\n",
    "    \n",
    "    def forward2(self, values, keys, queries, mask):\n",
    "        \"\"\"\n",
    "        values: (N, value_len, embed_size)\n",
    "        keys: (N, keys_len, embed_size)\n",
    "        queries: (N, queries_len, embed_size)\n",
    "\n",
    "        mask: None or (N, heads, query_len, key_len) \n",
    "        if mask == 0, attention matrix  -> float(\"-1e20\") (big negative value)\n",
    "        Ignore the mask, it's too difficult at the beginning.\n",
    "        \"\"\"\n",
    "        N = queries.shape[0]\n",
    "\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "        # Compute the values keys and queries\n",
    "        values = self.values(values)\n",
    "        queries = self.queries(queries)\n",
    "        keys = self.keys(keys)\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "\n",
    "        # Compute the similarity between query*keys\n",
    "        # Einsum does matrix mult. for query*keys for each training example\n",
    "        # with every other training example, don't be confused by einsum\n",
    "        # it's just how I like doing matrix multiplication & bmm\n",
    "        energy = torch.einsum(\"Nqhd,Nkhd -> Nhqk\" , [queries, keys])\n",
    "        # queries shape: (N, query_len, heads, heads_dim),\n",
    "        # keys shape: (N, key_len, heads, heads_dim)\n",
    "        # energy: (N, heads, query_len, key_len)\n",
    "\n",
    "\n",
    "        # Mask padded indices so their weights become 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "\n",
    "        # Normalize energy values similarly to seq2seq + attention\n",
    "        # so that they sum to 1. Also divide by scaling factor for better stability\n",
    "        attention = torch.softmax(energy / (self.head_dim ** (1 / 2)), dim=-1)\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "\n",
    "\n",
    "        # We compute the attention\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "        # values shape: (N, value_len, heads, heads_dim)\n",
    "        # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
    "        # we reshape and flatten the last two dimensions.\n",
    "\n",
    "\n",
    "        # Linear layer doesn't modify the shape, final shape will be\n",
    "        # (N, query_len, embed_size)\n",
    "        return self.fc_out(out)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        \"\"\"Reproduce the above figure.\n",
    "        \n",
    "        Tip: Dropout is always used after LayerNorm\n",
    "        \"\"\"\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        # Add skip connection, run through normalization and finally dropout\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "    \n",
    "    def forward2(self, value, key, query, mask):\n",
    "        \"\"\"Reproduce the above figure.\n",
    "        \n",
    "        Tip: Dropout is always used after LayerNorm\n",
    "        \"\"\"\n",
    "        attention = self.attention(values, key, query, mask)\n",
    "        attention_res_norm = self.dropout(self.norm1(attention + query))\n",
    "        out = self.norm2(self.feed_forward(attention_res_norm) + attention_res_norm)\n",
    "        return self.dropout(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length,\n",
    "    ):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        x: Tokenized tensor (N, seq_length) containing tokens_ids\n",
    "        mask: Used for masking the padding inside the encoder.\n",
    "\n",
    "        Create the position_embedding/word_embedding\n",
    "        add the embeddings and forward it the all the layers.\n",
    "\n",
    "        Tip: In order to create the position_embedding, you will need torch.arange and tensor.expand\n",
    "        \"\"\"\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        position_embedding = self.position_embedding(positions)\n",
    "        word_embedding = self.word_embedding(x)\n",
    "        out = self.dropout(position_embedding + word_embedding)\n",
    "\n",
    "        # In the Encoder the query, key, value are all the same, it's in the\n",
    "        # decoder this will change. This might look a bit odd in this case.\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def forward2(self, x, mask):\n",
    "        \"\"\"\n",
    "        x: Tokenized tensor (N, seq_length) containing tokens_ids\n",
    "        mask: Used for masking the padding inside the encoder.\n",
    "\n",
    "        Create the position_embedding/word_embedding\n",
    "        add the embeddings and forward it the all the layers.\n",
    "\n",
    "        Tip: In order to create the position_embedding, you will need torch.arange and tensor.expand\n",
    "        \"\"\"\n",
    "        N, seq_len = x.shape\n",
    "        pos_emb = self.position_embedding(torch.arange(seq_len).expand(N,seq_len).to(self.device))\n",
    "        out = (self.word_embedding(x) + pos_emb)\n",
    "        for transformer in self.layers:\n",
    "          out = transformer(out, out, out, mask) \n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = SelfAttention(embed_size, heads=heads)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        \"\"\"DecoderBlock = masked multi-head attention + TransformerBlock\"\"\"\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out\n",
    "    \n",
    "    def forward2(self, x, value, key, src_mask, trg_mask):\n",
    "        \"\"\"DecoderBlock = masked multi-head attention + TransformerBlock\"\"\"\n",
    "        attention = self.attention(x,x,x, trg_mask)\n",
    "        query = self.dropout(self.norm(attention+x))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        \"\"\"Same as Encoder\"\"\"\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward2(self, x, enc_out, src_mask, trg_mask):\n",
    "        \"\"\"Same as Encoder\"\"\"\n",
    "        N, seq_len = x.shape\n",
    "        pos_emb = self.position_embeddinq(torch.arange(seq_len).expand(N,seq_len).to(self.device))\n",
    "        word_emb = self.word_embedding(x)\n",
    "        out = self.dropout(pos_emb + word_emb)\n",
    "        for layer in self.layers:\n",
    "          out = layer(out, enc_out, enc_out, src_mask, trg_mask)\n",
    "        return self.fc_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        embed_size=512,\n",
    "        num_layers=6,\n",
    "        forward_expansion=4,\n",
    "        heads=8,\n",
    "        dropout=0,\n",
    "        device=\"cpu\",\n",
    "        max_length=100,\n",
    "    ):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        \"\"\"src is a tensor containing sequences of tokens. Some sequences have been padded.\n",
    "        \n",
    "        The purpose of the src_mask is to mask those padded tokens.\n",
    "        This mask is used both during training and inference time.\n",
    "        \"\"\"\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \"\"\"trg is a tensor containing sequences of tokens which have been predicted.\n",
    "\n",
    "        trg mask is used only during training.\n",
    "        \"\"\"\n",
    "        # Bonus: Why do we use a lower triangular matrix?\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.Size([2, 7, 10])\n",
      "tensor([[[-0.4929, -0.2060,  0.5872, -0.0701,  0.5324, -0.1011, -0.9801,\n",
      "           0.0660, -0.7428,  0.5581],\n",
      "         [ 0.5251, -0.6024, -0.2253, -0.6102,  0.2870, -0.3102, -0.3985,\n",
      "           0.6165,  0.0124,  0.9235],\n",
      "         [-0.1050, -0.1528, -1.0784, -0.3825,  0.0509, -0.4631, -0.9524,\n",
      "          -0.0348, -0.1872, -0.5002],\n",
      "         [ 1.1014,  0.1605, -0.4269, -0.7999,  0.1694, -0.8911, -0.2104,\n",
      "           0.3796, -1.1400,  0.0751],\n",
      "         [ 0.7232, -0.2951,  0.5434,  0.7596,  0.1499, -0.3296, -0.2009,\n",
      "          -0.0518, -0.3481,  0.2824],\n",
      "         [-0.0966, -0.1733, -0.1517, -0.1718,  0.5141,  0.5296,  0.2560,\n",
      "           0.8363, -0.6432,  0.1255],\n",
      "         [ 1.2186, -0.3571, -0.3976, -0.0792, -0.3543, -0.1938,  0.1148,\n",
      "           0.4604, -1.0823, -0.1248]],\n",
      "\n",
      "        [[-0.5025, -0.1797,  0.7957, -0.0449,  0.6293,  0.0666, -0.7979,\n",
      "          -0.0469, -0.6296,  0.5847],\n",
      "         [ 0.4198, -0.4763,  0.5345,  0.4182,  0.4071,  0.0438, -0.1908,\n",
      "           0.2302, -0.0757,  0.1884],\n",
      "         [-0.0748,  0.2244,  0.0511,  0.0342,  1.0775,  0.2395, -0.3518,\n",
      "          -0.2268, -0.0885, -0.3471],\n",
      "         [ 0.6518, -0.1845, -0.3614, -0.2620,  0.0136, -0.2536,  0.0547,\n",
      "           0.4172, -0.9061,  0.2524],\n",
      "         [-0.0202, -0.2468, -0.9430,  0.0250,  0.1949, -0.5670, -0.1207,\n",
      "           0.0058, -0.0724,  0.2241],\n",
      "         [ 0.4080,  0.3786, -0.0770, -0.8715,  0.1128,  0.3889,  0.6150,\n",
      "           0.5732, -0.1768,  0.5503],\n",
      "         [ 0.7075, -0.0224,  0.0254, -0.1557,  0.0546,  0.2626,  0.1932,\n",
      "           0.2912, -0.6336, -0.7172]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# 0 : pad, 1: Start of Sentence, 2: End of sentence\n",
    "# Those lists of token_ids are created by the tokenizer\n",
    "x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(\n",
    "    device\n",
    ")\n",
    "trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
    "\n",
    "src_pad_idx = 0\n",
    "trg_pad_idx = 0\n",
    "src_vocab_size = 10\n",
    "trg_vocab_size = 10\n",
    "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "# Why do we need :-1 here?\n",
    "out = model(x, trg[:, :-1])\n",
    "print(out.shape)\n",
    "\n",
    "# Then we copmpute the CrossEntropy between the output and the trg[:, 1:]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:00<00:09,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epochs: 0\n",
      "batch_num:0\n",
      "loss:2.429459810256958\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.98it/s]\n",
      "  3%|▎         | 3/100 [00:00<00:09, 10.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epochs: 1\n",
      "batch_num:0\n",
      "loss:1.5024415254592896\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 11.22it/s]\n",
      "  2%|▏         | 2/100 [00:00<00:08, 12.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epochs: 2\n",
      "batch_num:0\n",
      "loss:0.3801421821117401\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 11.11it/s]\n",
      "  2%|▏         | 2/100 [00:00<00:10,  9.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epochs: 3\n",
      "batch_num:0\n",
      "loss:0.026048433035612106\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 11.17it/s]\n",
      "  2%|▏         | 2/100 [00:00<00:08, 11.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epochs: 4\n",
      "batch_num:0\n",
      "loss:0.009326520375907421\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.12it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 84.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 8, 3, 8, 5, 7, 4, 9, 3, 3],\n",
      "        [3, 4, 6, 5, 8, 7, 5, 4, 9, 7]])\n",
      "tensor([[7, 8, 3, 8, 5, 7, 4, 9, 3, 3],\n",
      "        [3, 4, 6, 5, 8, 7, 5, 4, 9, 7]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "param = {\"batch_size\": 100, \n",
    "         \"src_pad_idx\": 0 , \n",
    "         \"trg_pad_idx\": 0,\n",
    "         \"num_train_data\": 10000,\n",
    "         \"seq_len\":10,\n",
    "         \"vocab_size\": 10, \n",
    "         \"epochs\": 5, \n",
    "         \"num_eval_data\": 2,\n",
    "         \"embed_size\":20,\n",
    "         \"num_layers\":6,\n",
    "         \"forward_expansion\":4,\n",
    "         \"heads\":5,\n",
    "         \"dropout\":0,\n",
    "         \"device\":device,\n",
    "         \"max_length\":100,}\n",
    "\n",
    "sequences_train = torch.randint(3,param[\"vocab_size\"],(param[\"num_train_data\"], param[\"seq_len\"]) )\n",
    "sequences_eval = torch.randint(3,param[\"vocab_size\"],(param[\"num_eval_data\"], param[\"seq_len\"]) )\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    # Take a tensor of tokens of size (num_seq, seq_len).\n",
    "    # Add a init token to each sequences.\n",
    "    # When we call __getitem__, return the sequences and the sequences flipped.\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.cat((torch.ones(1, dtype=torch.long),self.sequences[idx])), torch.cat((torch.ones(1, dtype=torch.long),torch.flip(self.sequences[idx], dims=(0,))))\n",
    "\n",
    "\n",
    "training_data = SequenceDataset(sequences_train)\n",
    "train_dataloader = DataLoader(training_data, batch_size=param[\"batch_size\"])\n",
    "\n",
    "eval_data = SequenceDataset(sequences_eval)\n",
    "eval_dataloader = DataLoader(eval_data, batch_size=param[\"batch_size\"])\n",
    "\n",
    "model = Transformer(\n",
    "    param[\"vocab_size\"], \n",
    "    param[\"vocab_size\"], \n",
    "    param[\"src_pad_idx\"], \n",
    "    param[\"trg_pad_idx\"], \n",
    "    embed_size=param[\"embed_size\"],\n",
    "    num_layers=param[\"num_layers\"],\n",
    "    forward_expansion=param[\"forward_expansion\"],\n",
    "    heads=param[\"heads\"],\n",
    "    dropout=param[\"dropout\"],\n",
    "    device=param[\"device\"],\n",
    "    max_length=param[\"max_length\"]).to(\n",
    "    device\n",
    ")\n",
    "model.train()\n",
    "loss_cross = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "errors =  []\n",
    "for epoch in range(param[\"epochs\"]):\n",
    "  for batch, (X, y) in  enumerate(tqdm(train_dataloader)):\n",
    "    X, y = X.to(device), y.to(device)\n",
    "\n",
    "    # Compute prediction error\n",
    "    pred = model(X, y[:,:-1])\n",
    "    loss = loss_cross(pred.reshape(param[\"batch_size\"]*(param[\"seq_len\"]), param[\"vocab_size\"]), y[:,1:].reshape(param[\"batch_size\"]*(param[\"seq_len\"])))\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch%100 == 0:\n",
    "      errors.append(loss.item())\n",
    "      print(f\"\\nepochs: {epoch}\\nbatch_num:{batch}\\nloss:{loss}\\n\")\n",
    "\n",
    "plt.plot(errors)\n",
    "plt.title(\"Errors\")\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "for batch, (X, y) in  enumerate(tqdm(eval_dataloader)):\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    pred = torch.argmax(model(X, y[:,:-1]),dim=2)\n",
    "    print(y[:,1:])\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/tmp/ipykernel_44814/3129857807.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(torch.nn.functional.softmax(pred))\n",
      "100%|██████████| 1/1 [00:00<00:00, 30.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 3, 3, 9, 4, 7, 5, 8, 3, 8, 7],\n",
      "        [1, 7, 9, 4, 5, 7, 8, 5, 6, 4, 3]])\n",
      "tensor([[7, 8, 3, 8, 5, 7, 4, 9, 3, 3],\n",
      "        [3, 4, 6, 5, 8, 7, 5, 4, 9, 7]])\n",
      "tensor([[[1.8176e-01, 6.4254e-01, 1.6522e-01, 1.2136e-04, 4.3537e-02,\n",
      "          7.0606e-01, 2.9606e-01, 9.9992e-01, 8.9183e-01, 2.9181e-01],\n",
      "         [4.9544e-01, 7.8169e-01, 6.3187e-01, 1.8668e-01, 8.9913e-04,\n",
      "          7.7026e-01, 3.1489e-01, 9.4411e-01, 9.9943e-01, 2.1859e-01],\n",
      "         [4.7769e-01, 4.2063e-01, 8.1681e-01, 9.9970e-01, 5.8027e-01,\n",
      "          5.8415e-01, 7.0141e-04, 4.1148e-01, 2.7014e-01, 7.4416e-01],\n",
      "         [4.2702e-01, 7.9026e-01, 4.3642e-01, 5.2052e-01, 8.5892e-01,\n",
      "          1.0208e-03, 5.4335e-01, 3.0328e-01, 9.9957e-01, 5.1277e-01],\n",
      "         [5.0103e-01, 1.9441e-01, 4.2675e-01, 2.6001e-01, 3.1462e-01,\n",
      "          9.9683e-01, 2.5809e-01, 8.6456e-01, 5.8741e-04, 6.1985e-01],\n",
      "         [4.9488e-01, 6.1267e-01, 6.2639e-01, 8.6887e-01, 5.6579e-01,\n",
      "          1.8601e-01, 4.5139e-01, 4.6417e-01, 7.9582e-01, 2.5556e-01],\n",
      "         [2.7840e-01, 4.3375e-01, 3.1382e-01, 9.2333e-01, 9.9956e-01,\n",
      "          2.0325e-04, 4.1277e-01, 4.3244e-01, 3.7648e-01, 7.0189e-01],\n",
      "         [6.5160e-01, 4.6637e-01, 5.9991e-01, 8.9623e-01, 9.3359e-04,\n",
      "          2.2296e-01, 4.6003e-01, 6.6150e-01, 5.0076e-02, 9.9829e-01],\n",
      "         [6.3659e-01, 5.9362e-01, 8.5608e-01, 9.9990e-01, 5.1180e-01,\n",
      "          4.9477e-01, 9.5901e-01, 2.6032e-02, 7.9990e-01, 1.7747e-03],\n",
      "         [8.0148e-01, 4.7497e-01, 8.3055e-01, 9.9945e-01, 9.3005e-01,\n",
      "          4.1090e-01, 6.9390e-01, 2.9675e-04, 5.8251e-01, 3.2459e-01]],\n",
      "\n",
      "        [[8.1824e-01, 3.5746e-01, 8.3478e-01, 9.9988e-01, 9.5646e-01,\n",
      "          2.9394e-01, 7.0394e-01, 8.2656e-05, 1.0817e-01, 7.0819e-01],\n",
      "         [5.0456e-01, 2.1831e-01, 3.6813e-01, 8.1332e-01, 9.9910e-01,\n",
      "          2.2974e-01, 6.8511e-01, 5.5895e-02, 5.7326e-04, 7.8141e-01],\n",
      "         [5.2231e-01, 5.7937e-01, 1.8319e-01, 2.9943e-04, 4.1973e-01,\n",
      "          4.1585e-01, 9.9930e-01, 5.8852e-01, 7.2986e-01, 2.5584e-01],\n",
      "         [5.7298e-01, 2.0974e-01, 5.6358e-01, 4.7948e-01, 1.4108e-01,\n",
      "          9.9898e-01, 4.5665e-01, 6.9672e-01, 4.3046e-04, 4.8723e-01],\n",
      "         [4.9896e-01, 8.0559e-01, 5.7325e-01, 7.3999e-01, 6.8538e-01,\n",
      "          3.1668e-03, 7.4191e-01, 1.3544e-01, 9.9941e-01, 3.8015e-01],\n",
      "         [5.0512e-01, 3.8733e-01, 3.7361e-01, 1.3113e-01, 4.3421e-01,\n",
      "          8.1399e-01, 5.4861e-01, 5.3583e-01, 2.0418e-01, 7.4444e-01],\n",
      "         [7.2160e-01, 5.6625e-01, 6.8618e-01, 7.6668e-02, 4.3645e-04,\n",
      "          9.9980e-01, 5.8723e-01, 5.6756e-01, 6.2352e-01, 2.9811e-01],\n",
      "         [3.4840e-01, 5.3363e-01, 4.0009e-01, 1.0377e-01, 9.9907e-01,\n",
      "          7.7704e-01, 5.3997e-01, 3.3850e-01, 9.4992e-01, 1.7093e-03],\n",
      "         [3.6341e-01, 4.0638e-01, 1.4392e-01, 1.0113e-04, 4.8820e-01,\n",
      "          5.0523e-01, 4.0986e-02, 9.7397e-01, 2.0010e-01, 9.9823e-01],\n",
      "         [1.9852e-01, 5.2503e-01, 1.6945e-01, 5.5417e-04, 6.9948e-02,\n",
      "          5.8910e-01, 3.0610e-01, 9.9970e-01, 4.1749e-01, 6.7541e-01]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[7, 8, 3, 8, 5, 7, 4, 9, 3, 3],\n",
      "        [3, 4, 6, 5, 8, 7, 5, 4, 9, 7]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for batch, (X, y) in  enumerate(tqdm(eval_dataloader)):\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    pred = model(X, y[:,:-1])\n",
    "    print(X)\n",
    "    print(y[:,1:])\n",
    "    print(torch.nn.functional.softmax(pred))\n",
    "    print(torch.argmax(pred,dim=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
