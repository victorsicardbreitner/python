{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers_solution\n",
    "\n",
    "https://github.com/EffiSciencesResearch/ML4G/blob/main/days/w1d2/optimizers/optimizers_solution.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing test_sgd ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/python/Deep Learning Pytorch/ML4G/optimizers_solution.ipynb Cellule 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Brefactored-cod-p6q4rjgvrwp3rw74/workspaces/python/Deep%20Learning%20Pytorch/ML4G/optimizers_solution.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=251'>252</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Brefactored-cod-p6q4rjgvrwp3rw74/workspaces/python/Deep%20Learning%20Pytorch/ML4G/optimizers_solution.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=252'>253</a>\u001b[0m \u001b[39mBonus: \u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Brefactored-cod-p6q4rjgvrwp3rw74/workspaces/python/Deep%20Learning%20Pytorch/ML4G/optimizers_solution.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=253'>254</a>\u001b[0m \u001b[39m- Give a reason to use SGD instead of Adam.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Brefactored-cod-p6q4rjgvrwp3rw74/workspaces/python/Deep%20Learning%20Pytorch/ML4G/optimizers_solution.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=256'>257</a>\u001b[0m \u001b[39m- What is a Parent class? Modify the script to use a Parent class \u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Brefactored-cod-p6q4rjgvrwp3rw74/workspaces/python/Deep%20Learning%20Pytorch/ML4G/optimizers_solution.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=257'>258</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Brefactored-cod-p6q4rjgvrwp3rw74/workspaces/python/Deep%20Learning%20Pytorch/ML4G/optimizers_solution.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=259'>260</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell://codespaces%2Brefactored-cod-p6q4rjgvrwp3rw74/workspaces/python/Deep%20Learning%20Pytorch/ML4G/optimizers_solution.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=260'>261</a>\u001b[0m     tests\u001b[39m.\u001b[39;49mtest_sgd(_SGD)\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Brefactored-cod-p6q4rjgvrwp3rw74/workspaces/python/Deep%20Learning%20Pytorch/ML4G/optimizers_solution.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=261'>262</a>\u001b[0m     tests\u001b[39m.\u001b[39mtest_adam(_Adam)\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Brefactored-cod-p6q4rjgvrwp3rw74/workspaces/python/Deep%20Learning%20Pytorch/ML4G/optimizers_solution.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=262'>263</a>\u001b[0m     tests\u001b[39m.\u001b[39mtest_rmsprop(_RMSprop)\n",
      "File \u001b[0;32m/workspaces/python/Deep Learning Pytorch/ML4G/optimizers_tests.py:35\u001b[0m, in \u001b[0;36mname_function.<locals>.timed\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtimed\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mExecuting\u001b[39m\u001b[39m'\u001b[39m, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     36\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/workspaces/python/Deep Learning Pytorch/ML4G/optimizers_tests.py:145\u001b[0m, in \u001b[0;36mtest_sgd\u001b[0;34m(SGD)\u001b[0m\n\u001b[1;32m    143\u001b[0m model \u001b[39m=\u001b[39m sol\u001b[39m.\u001b[39m_MLP(\u001b[39m2\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    144\u001b[0m opt \u001b[39m=\u001b[39m SGD(model\u001b[39m.\u001b[39mparameters(), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopt_config)\n\u001b[0;32m--> 145\u001b[0m _train_with_opt(model, opt)\n\u001b[1;32m    146\u001b[0m w0_submitted \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mweight\n\u001b[1;32m    148\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mTesting configuration: \u001b[39m\u001b[39m\"\u001b[39m, opt_config)\n",
      "File \u001b[0;32m/workspaces/python/Deep Learning Pytorch/ML4G/optimizers_tests.py:124\u001b[0m, in \u001b[0;36m_train_with_opt\u001b[0;34m(model, opt)\u001b[0m\n\u001b[1;32m    122\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(model(X), y)\n\u001b[1;32m    123\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m--> 124\u001b[0m opt\u001b[39m.\u001b[39;49mstep()\n",
      "\u001b[1;32m/workspaces/python/Deep Learning Pytorch/ML4G/optimizers_solution.ipynb Cellule 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Brefactored-cod-p6q4rjgvrwp3rw74/workspaces/python/Deep%20Learning%20Pytorch/ML4G/optimizers_solution.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=126'>127</a>\u001b[0m g \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mgrad \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwd \u001b[39m*\u001b[39m p\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Brefactored-cod-p6q4rjgvrwp3rw74/workspaces/python/Deep%20Learning%20Pytorch/ML4G/optimizers_solution.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=128'>129</a>\u001b[0m \u001b[39m# momentum\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://codespaces%2Brefactored-cod-p6q4rjgvrwp3rw74/workspaces/python/Deep%20Learning%20Pytorch/ML4G/optimizers_solution.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=129'>130</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb[i]:\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Brefactored-cod-p6q4rjgvrwp3rw74/workspaces/python/Deep%20Learning%20Pytorch/ML4G/optimizers_solution.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=130'>131</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb[i] \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdampening) \u001b[39m*\u001b[39m g\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Brefactored-cod-p6q4rjgvrwp3rw74/workspaces/python/Deep%20Learning%20Pytorch/ML4G/optimizers_solution.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=131'>132</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "\"\"\"In this script, we implement 3 optimizers\n",
    "\n",
    "Rad this article:\n",
    "https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c\n",
    "\n",
    "Then try to implement them here.\n",
    "\n",
    "Some details are omitted, there is little chance that you will pass the tests. So read the solution after 5 minutes of try & error\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Tuple\n",
    "import optimizers_tests as tests\n",
    "\n",
    "\n",
    "class _MLP(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "def _train(model: nn.Module, dataloader: DataLoader, lr, momentum):\n",
    "    opt = torch.optim.SGD(model.parameters(), lr, momentum)\n",
    "    for X, y in dataloader:\n",
    "        opt.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = F.l1_loss(pred, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    return model\n",
    "\n",
    "\n",
    "def _accuracy(model: nn.Module, dataloader: DataLoader):\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    for X, y in dataloader:\n",
    "        n_correct += (model(X).argmax(1) == y).sum().item()\n",
    "        n_total += len(y)\n",
    "    return n_correct / n_total\n",
    "\n",
    "\n",
    "def _evaluate(model: nn.Module, dataloader: DataLoader):\n",
    "    sum_abs = 0.0\n",
    "    n_elems = 0\n",
    "    for X, y in dataloader:\n",
    "        sum_abs += (model(X) - y).abs().sum()\n",
    "        n_elems += y.shape[0] * y.shape[1]\n",
    "    return sum_abs / n_elems\n",
    "\n",
    "\n",
    "def _rosenbrock(x, y, a=1, b=100):\n",
    "    return (a - x) ** 2 + b * (y - x**2) ** 2 + 1\n",
    "\n",
    "\n",
    "def _opt_rosenbrock(xy, lr, momentum, n_iter):\n",
    "    w_history = torch.zeros([n_iter + 1, 2])\n",
    "    w_history[0] = xy.detach()\n",
    "    opt = torch.optim.SGD([xy], lr=lr, momentum=momentum)\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        opt.zero_grad()\n",
    "        _rosenbrock(xy[0], xy[1]).backward()\n",
    "\n",
    "        opt.step()\n",
    "        w_history[i + 1] = xy.detach()\n",
    "    return w_history\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Generalities:\n",
    "Read the _opt_rosenbrock code.\n",
    "Why do we have to zero the gradient in PyTorch? \n",
    "Why do we use the word 'stochastic' in 'Stochastic gradient descent' in the context of deep learning?\n",
    "\n",
    "SGD:\n",
    "Please don't look back at the article. We will try to construct the formula ourself here.\n",
    "Below, read the method zero_grad. You can note that in PyTorch, to zero a gradient means assigning None.\n",
    "\n",
    "We will implement successively the pure SGD, the sgd with weight decay, and the SGD with momentum.\n",
    "\n",
    "Implement steps:\n",
    "    - # update: Implement the most basic version of SGD possible\n",
    "    - Why do we need the 'with torch.no_grad()' context manager?\n",
    "    - # weight_decay: What is the formula of the update when there is some weight_decay (ie when we penalize each parameter squared)? Assume wd absorbs the constant.\n",
    "        Tip: let's say we optimize L(X, y) = (ax_1 + bx_2 + c - y)^2 with respect to a, b and c.\n",
    "        Adding weight_decay means that instead of minimizing L, we minimize g(X, y) =  L(X, y) + wd(a^2 + b^2 + c^2)/2\n",
    "        For this example, what is the formula of the gradient wrt a,b and c?\n",
    "        In the code, at the beginning of the step function, replace the gradient by g = p.grad + self.wd * p\n",
    "    - # momentum: Why do we need self.b in the __init__? Add momentum. Separate the cases self.momentum equals zero or not.\n",
    "\n",
    "There are multiple ways to implement SGD, so don't panic if there is some ambiguity and look at the solution to compare with the PyTorch implementation when you have used every variable.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class _SGD:\n",
    "    def __init__(\n",
    "        self, params, lr: float, momentum: float, dampening: float, weight_decay: float\n",
    "    ):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.wd = weight_decay\n",
    "        self.momentum = momentum  # here, it's the correct definition of momentum\n",
    "        self.dampening = dampening  # generally 1-momentum = 1-dampening\n",
    "        self.b = [None for _ in self.params]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad = None\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for i, p in enumerate(self.params):\n",
    "                assert p.grad is not None, \"Don't forget loss.backward()!\"\n",
    "                # weight decay\n",
    "                g = p.grad + self.wd * p\n",
    "\n",
    "                # momentum\n",
    "                if self.b[i]:\n",
    "                    self.b[i] = self.momentum * self.b[i] + (1 - self.dampening) * g\n",
    "                else:\n",
    "                    self.b[i] = g\n",
    "\n",
    "                # update\n",
    "                p -= self.b[i] * self.lr\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Adam, by far the most used optimizer.\n",
    "\n",
    "It's a combination of SGD+RMSProps and uses one momentum for the gradient, and another for the gradient squared.\n",
    "\n",
    "1. Adam\n",
    "Tip: There is no if condition in Adam because beta1 and beta2 are always stricly positive.\n",
    "sum_of_gradient = previous_sum_of_gradient * beta1 + gradient * (1 - beta1) [SGD+Momentum]\n",
    "sum_of_gradient_squared = previous_sum_of_gradient_squared * beta2 + gradient² * (1- beta2) [RMSProp]\n",
    "delta = -learning_rate * sum_of_gradient / sqrt(sum_of_gradient_squared)\n",
    "Update the gradient\n",
    "\n",
    "2. More stability + regularization\n",
    "Add self.eps to the denominator, outside the square root.\n",
    "At the beginning of the step function, replace the gradient by g = p.grad + self.wd * p\n",
    "\n",
    "3. Adam + Correction\n",
    "In the __init__, self.b1 and self.b2 are a list of zeros, so we need a correction:\n",
    "In the update, use a correction: b1_hat = self.b1[i] / (1.0 - self.beta1**self.t)\n",
    "Generally beta1 = 0.9, and beta2 = 0.999.\n",
    "Same for b2_hat.\n",
    "NB: The correction is important only at the beginning of the optimization process (self.t small).\n",
    "The the correction is needed because self.b1 and self.b2 are initialized at zero.\n",
    "\n",
    "Try to pass the tests.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class _Adam:\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float,\n",
    "        betas: Tuple[float, float],\n",
    "        eps: float,\n",
    "        weight_decay: float,\n",
    "    ):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.beta1, self.beta2 = betas  # momenti of b1 and b2\n",
    "        self.eps = eps\n",
    "        self.wd = weight_decay\n",
    "\n",
    "        self.b1 = [torch.zeros_like(p) for p in self.params]\n",
    "        self.b2 = [torch.zeros_like(p) for p in self.params]\n",
    "        self.t = 0\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad = None\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        with torch.no_grad():\n",
    "            for i, p in enumerate(self.params):\n",
    "                assert p.grad is not None\n",
    "                g = p.grad + self.wd * p\n",
    "                self.b1[i] = self.beta1 * self.b1[i] + (1.0 - self.beta1) * g\n",
    "                self.b2[i] = self.beta2 * self.b2[i] + (1.0 - self.beta2) * g**2\n",
    "                b1_hat = self.b1[i] / (1.0 - self.beta1**self.t)\n",
    "                b2_hat = self.b2[i] / (1.0 - self.beta2**self.t)\n",
    "                p -= self.lr * b1_hat / (b2_hat.sqrt() + self.eps)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "(Bonus) _RMSprop: Using the square of the gradient to adapt the lr\n",
    "- What is the formula of the update when there is some weight_decay? Assume wd absorbs the constant.\n",
    "- Update the squared gradient.\n",
    "- Why do we use the gradient squared? Why do we say that the lr in _RMSprop is adaptive?\n",
    "- eps should be outside the squared root\n",
    "- Separate the cases self.momentum zero or not.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class _RMSprop:\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float,\n",
    "        alpha: float,\n",
    "        eps: float,\n",
    "        weight_decay: float,\n",
    "        momentum: float,\n",
    "    ):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha  # momentum of gradient squared\n",
    "        self.eps = eps\n",
    "        self.wd = weight_decay\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.b2 = [torch.zeros_like(p) for p in self.params]  # gradient squared\n",
    "        self.b = [torch.zeros_like(p) for p in self.params]  # gradient\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad = None\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for i, p in enumerate(self.params):\n",
    "                assert p.grad is not None\n",
    "                g = p.grad + self.wd * p\n",
    "                self.b2[i] = self.alpha * self.b2[i] + (1.0 - self.alpha) * g**2\n",
    "                if self.momentum:\n",
    "                    self.b[i] = self.momentum * self.b[i] + g / (\n",
    "                        self.b2[i].sqrt() + self.eps\n",
    "                    )\n",
    "                    p -= self.lr * self.b[i]\n",
    "                else:\n",
    "                    p -= self.lr * g / (self.b2[i].sqrt() + self.eps)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Bonus: \n",
    "- Give a reason to use SGD instead of Adam.\n",
    "- What is an abstract class in python?\n",
    "- Modify the script to use an abstract class.\n",
    "- What is a Parent class? Modify the script to use a Parent class \n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tests.test_sgd(_SGD)\n",
    "    tests.test_adam(_Adam)\n",
    "    tests.test_rmsprop(_RMSprop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
