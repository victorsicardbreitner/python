{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 07:27:28.915444: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-18 07:27:28.947215: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-18 07:27:28.947262: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-18 07:27:28.947297: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-18 07:27:28.954646: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-18 07:27:28.955449: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-18 07:27:30.720624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.framework.ops' has no attribute 'register_tensor_conversion_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/python/Deep Learning Pytorch/ML4G/Tensor2tensor/mtf_image_transformer.ipynb Cellule 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Brefactored-cod-p6q4rjgvrwp3rw74/workspaces/python/Deep%20Learning%20Pytorch/ML4G/Tensor2tensor/mtf_image_transformer.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m print_function\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Brefactored-cod-p6q4rjgvrwp3rw74/workspaces/python/Deep%20Learning%20Pytorch/ML4G/Tensor2tensor/mtf_image_transformer.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcopy\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Brefactored-cod-p6q4rjgvrwp3rw74/workspaces/python/Deep%20Learning%20Pytorch/ML4G/Tensor2tensor/mtf_image_transformer.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmesh_tensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmtf\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Brefactored-cod-p6q4rjgvrwp3rw74/workspaces/python/Deep%20Learning%20Pytorch/ML4G/Tensor2tensor/mtf_image_transformer.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensor2tensor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m common_hparams\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Brefactored-cod-p6q4rjgvrwp3rw74/workspaces/python/Deep%20Learning%20Pytorch/ML4G/Tensor2tensor/mtf_image_transformer.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensor2tensor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m common_layers\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/mesh_tensorflow/__init__.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmesh_tensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m optimize\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmesh_tensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m placement_mesh_impl\n\u001b[0;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmesh_tensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m simd_mesh_impl\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmesh_tensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m tpu_variables\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmesh_tensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/mesh_tensorflow/simd_mesh_impl.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgin\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmesh_tensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m ops_with_redefined_builtins \u001b[39mas\u001b[39;00m mtf\n\u001b[0;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmesh_tensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m tpu_variables\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmesh_tensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msix\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmoves\u001b[39;00m \u001b[39mimport\u001b[39;00m xrange  \u001b[39m# pylint: disable=redefined-builtin\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/mesh_tensorflow/tpu_variables.py:226\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_tensor_conversion\u001b[39m(var, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    223\u001b[0m   \u001b[39mreturn\u001b[39;00m var\u001b[39m.\u001b[39m_dense_var_to_tensor(dtype\u001b[39m=\u001b[39mdtype, name\u001b[39m=\u001b[39mname, as_ref\u001b[39m=\u001b[39mas_ref)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m ops\u001b[39m.\u001b[39;49mregister_tensor_conversion_function(ReplicatedVariable, _tensor_conversion)\n\u001b[1;32m    228\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m TF_23:\n\u001b[1;32m    229\u001b[0m   ops\u001b[39m.\u001b[39mregister_dense_tensor_like_type(ReplicatedVariable)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.framework.ops' has no attribute 'register_tensor_conversion_function'"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2023 The Tensor2Tensor Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Image Transformer model with model and data parallelism using MTF.\n",
    "\n",
    "Integration of Mesh tensorflow with Image Transformer to do model parallelism.\n",
    "Currently, this supports unconditional image generation. Specify a particular\n",
    "architecture layout in the hparams that specifies how different dimensions are\n",
    "split or replicated along the mesh dimensions.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import copy\n",
    "import mesh_tensorflow as mtf\n",
    "\n",
    "from tensor2tensor.layers import common_hparams\n",
    "from tensor2tensor.layers import common_layers\n",
    "from tensor2tensor.utils import mtf_model\n",
    "from tensor2tensor.utils import registry\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.compat.v1 import estimator as tf_estimator\n",
    "\n",
    "\n",
    "@registry.register_model\n",
    "class MtfImageTransformer(mtf_model.MtfModel):\n",
    "  \"\"\"Image Transformer in mesh_tensorflow.\"\"\"\n",
    "\n",
    "  @property\n",
    "  def inputs_vocab_dim(self):\n",
    "    assert self.has_input\n",
    "    return mtf.Dimension(\"inputs_vocab\", self._hparams.num_classes)\n",
    "\n",
    "  @property\n",
    "  def targets_vocab_dim(self):\n",
    "    vocab_size = self._problem_hparams.vocab_size[\"targets\"]\n",
    "    if hasattr(self._hparams, \"vocab_divisor\"):\n",
    "      vocab_size += (-vocab_size) % self._hparams.vocab_divisor\n",
    "    return mtf.Dimension(\"vocab\", vocab_size)\n",
    "\n",
    "  @property\n",
    "  def outputs_vocab_dim(self):\n",
    "    return mtf.Dimension(\"output_vocab\", 256)\n",
    "\n",
    "  @property\n",
    "  def pos_dim(self):\n",
    "    return mtf.Dimension(\"pos\", self._hparams.img_len)\n",
    "\n",
    "  @property\n",
    "  def rows_dim(self):\n",
    "    return mtf.Dimension(\"rows\", self._hparams.img_len)\n",
    "\n",
    "  @property\n",
    "  def cols_dim(self):\n",
    "    return mtf.Dimension(\n",
    "        \"cols\", self._hparams.img_len*self._hparams.num_channels)\n",
    "\n",
    "  @property\n",
    "  def orig_cols_dim(self):\n",
    "    return mtf.Dimension(\"orig_cols\", self._hparams.img_len)\n",
    "\n",
    "  @property\n",
    "  def channels_dim(self):\n",
    "    return mtf.Dimension(\"channels\", self._hparams.num_channels)\n",
    "\n",
    "  @property\n",
    "  def model_dim(self):\n",
    "    return mtf.Dimension(\"d_model\", self._hparams.hidden_size)\n",
    "\n",
    "  @property\n",
    "  def max_length_dim(self):\n",
    "    return mtf.Dimension(\n",
    "        \"max_length\",\n",
    "        self._hparams.img_len*self._hparams.img_len*self._hparams.num_channels)\n",
    "\n",
    "  @property\n",
    "  def length_dim(self):\n",
    "    return mtf.Dimension(\n",
    "        \"length\",\n",
    "        self._hparams.img_len*self._hparams.img_len*self._hparams.num_channels)\n",
    "\n",
    "  @property\n",
    "  def heads_dim(self):\n",
    "    return mtf.Dimension(\"heads\", self._hparams.num_heads)\n",
    "\n",
    "  @property\n",
    "  def kv_dim(self):\n",
    "    return mtf.Dimension(\"d_kv\", self._hparams.d_kv)\n",
    "\n",
    "  @property\n",
    "  def feedforward_dim(self):\n",
    "    return mtf.Dimension(\"d_ff\", self._hparams.d_ff)\n",
    "\n",
    "  @property\n",
    "  def activation_type(self):\n",
    "    hparams = self._hparams\n",
    "    if hparams.activation_dtype == \"float32\":\n",
    "      activation_dtype = tf.float32\n",
    "    elif hparams.activation_dtype == \"float16\":\n",
    "      activation_dtype = tf.float16\n",
    "    elif hparams.activation_dtype == \"bfloat16\":\n",
    "      activation_dtype = tf.bfloat16\n",
    "    else:\n",
    "      raise ValueError(\n",
    "          \"unknown hparams.activation_dtype %s\" % hparams.activation_dtype)\n",
    "    return activation_dtype\n",
    "\n",
    "  def create_positional_emb_2d(self, targets):\n",
    "    \"\"\"Learned 2d positional embedding for images.\"\"\"\n",
    "    mesh = targets.mesh\n",
    "\n",
    "    positional_emb_rows_var = mtf.get_variable(\n",
    "        mesh, \"positional_emb_rows\",\n",
    "        mtf.Shape([self.pos_dim, self.model_dim]),\n",
    "        initializer=tf.random_normal_initializer(),\n",
    "        activation_dtype=self.activation_type)\n",
    "    positional_emb_cols_var = mtf.get_variable(\n",
    "        mesh, \"positional_emb_cols\",\n",
    "        mtf.Shape([self.pos_dim, self.model_dim]),\n",
    "        initializer=tf.random_normal_initializer(),\n",
    "        activation_dtype=self.activation_type)\n",
    "\n",
    "    targets_position_x = mtf.range(mesh, self.rows_dim, dtype=tf.int32)\n",
    "    targets_position_y = mtf.range(mesh, self.cols_dim, dtype=tf.int32)\n",
    "    position_x = mtf.broadcast(\n",
    "        mtf.gather(positional_emb_rows_var, targets_position_x,\n",
    "                   self.pos_dim),\n",
    "        mtf.Shape([self.rows_dim, self.cols_dim, self.model_dim]))\n",
    "\n",
    "    position_y = mtf.broadcast(\n",
    "        mtf.gather(positional_emb_cols_var, targets_position_y,\n",
    "                   self.pos_dim),\n",
    "        mtf.Shape([self.rows_dim, self.cols_dim, self.model_dim]))\n",
    "    return position_x + position_y\n",
    "\n",
    "  def mtf_model_fn(self, features, mesh):\n",
    "    features = copy.copy(features)\n",
    "    tf.logging.info(\"features = %s\" % features)\n",
    "    hparams = self._hparams\n",
    "    activation_dtype = self.activation_type\n",
    "\n",
    "    # We assume fixed vocab size for targets\n",
    "    targets = tf.to_int32(features[\"targets\"])\n",
    "\n",
    "    # Image preprocessing, reshape into a 1D sequence and shift right.\n",
    "    length = hparams.img_len*hparams.img_len*hparams.num_channels\n",
    "    targets = tf.reshape(targets, [hparams.batch_size, length])\n",
    "    shifted_targets = common_layers.shift_right_2d(targets)\n",
    "\n",
    "    # Declare all the dimensions\n",
    "    batch_dim = mtf.Dimension(\"batch\", hparams.batch_size)\n",
    "\n",
    "    def import_to_batch_by_length(x, name):\n",
    "      return mtf.import_tf_tensor(\n",
    "          mesh, x, mtf.Shape([batch_dim, self.length_dim]), name=name)\n",
    "\n",
    "    targets = import_to_batch_by_length(targets, \"targets\")\n",
    "    shifted_targets = import_to_batch_by_length(\n",
    "        shifted_targets, \"shifted_targets\")\n",
    "\n",
    "    extra_losses = []\n",
    "\n",
    "    # Create targets content and position embeddings.\n",
    "    # Create embedding var for targets and positions and do a gather.\n",
    "    targets_embedding_var = mtf.get_variable(\n",
    "        mesh, \"targets_embedding\",\n",
    "        mtf.Shape([self.targets_vocab_dim, self.model_dim]),\n",
    "        initializer=tf.random_normal_initializer(),\n",
    "        activation_dtype=activation_dtype)\n",
    "\n",
    "    x = mtf.gather(targets_embedding_var,\n",
    "                   shifted_targets, self.targets_vocab_dim)\n",
    "\n",
    "    # Add positional embeddings\n",
    "    x += mtf.reshape(self.create_positional_emb_2d(targets),\n",
    "                     [self.length_dim, self.model_dim])\n",
    "\n",
    "    # If conditional and input is given, add the input embedding to the target.\n",
    "    # TODO(nikip): Verify conditional.\n",
    "    if self.has_input and not hparams.unconditional:\n",
    "      inputs = tf.squeeze(tf.to_int32(features[\"inputs\"]), [2, 3])\n",
    "      inputs = import_to_batch_by_length(inputs, \"inputs\")\n",
    "\n",
    "      # Input embeddings\n",
    "      inputs_embedding_var = mtf.layers.embedding(\n",
    "          mesh, \"input_embedding\",\n",
    "          mtf.Shape([self.inputs_vocab_dim, self.model_dim]),\n",
    "          activation_dtype=activation_dtype)\n",
    "      inputs_emb = mtf.gather(\n",
    "          inputs_embedding_var, inputs, self.inputs_vocab_dim)\n",
    "      x += inputs_emb\n",
    "\n",
    "    # Image Transformer Decoder\n",
    "    # [ self attention - ffn - residual + dropout] x n\n",
    "    if hparams.attention_type == \"local1d_spatial\":\n",
    "      decoder_output = local_attention1d_spatial_decoder(\n",
    "          x, self.kv_dim, self.heads_dim, self.feedforward_dim, hparams)\n",
    "    elif hparams.attention_type == \"local2d_spatial\":\n",
    "      decoder_output = local_attention2d_spatial_decoder(\n",
    "          x, self.kv_dim, self.heads_dim, self.feedforward_dim, hparams)\n",
    "    elif hparams.attention_type == \"local1d\":\n",
    "      decoder_output = local_attention1d_masked_decoder(\n",
    "          x, self.kv_dim, self.heads_dim, self.feedforward_dim, hparams)\n",
    "    else:\n",
    "      raise ValueError(\"Invalid attention type.\")\n",
    "\n",
    "    # Calculate the logits and loss.\n",
    "    logits = mtf.layers.dense(\n",
    "        decoder_output, self.outputs_vocab_dim, name=\"logits\")\n",
    "    # Need a reshape for logits\n",
    "    logits = mtf.reshape(\n",
    "        logits, mtf.Shape([batch_dim, self.length_dim, self.outputs_vocab_dim]))\n",
    "    soft_targets = mtf.one_hot(\n",
    "        targets, self.outputs_vocab_dim, dtype=activation_dtype)\n",
    "    loss = mtf.layers.softmax_cross_entropy_with_logits(\n",
    "        logits, soft_targets, self.outputs_vocab_dim)\n",
    "    loss = mtf.reduce_mean(loss)\n",
    "    for l in extra_losses:\n",
    "      loss += l\n",
    "\n",
    "    # Reshape logits to original target shape.\n",
    "    logits = mtf.reshape(\n",
    "        logits,\n",
    "        mtf.Shape([batch_dim, self.rows_dim, self.orig_cols_dim,\n",
    "                   self.channels_dim, self.outputs_vocab_dim]))\n",
    "\n",
    "    return logits, loss\n",
    "\n",
    "\n",
    "def layer_prepostprocess_dropout(x, hparams):\n",
    "  batch_dim = x.shape.dims[0]\n",
    "  model_dim = x.shape.dims[-1]\n",
    "  mode = getattr(hparams, \"mode\", tf_estimator.ModeKeys.TRAIN)\n",
    "  is_training = mode == tf_estimator.ModeKeys.TRAIN\n",
    "  return mtf.dropout(\n",
    "      x, is_training,\n",
    "      keep_prob=1.0 - hparams.layer_prepostprocess_dropout,\n",
    "      noise_shape=mtf.Shape([batch_dim, model_dim]))\n",
    "\n",
    "\n",
    "def local_attention1d_spatial_decoder(x, kv_dim, heads_dim,\n",
    "                                      feedforward_dim, hparams):\n",
    "  \"\"\"Image Transformer decoder with local1D spatial layers.\"\"\"\n",
    "  batch_dim, length_dim, model_dim = x.shape.dims\n",
    "  blocks_w_dim = mtf.Dimension(\"blocksw\", hparams.block_length)\n",
    "  num_w_blocks_dim = mtf.Dimension(\"num_wblocks\",\n",
    "                                   length_dim.size // blocks_w_dim.size)\n",
    "  x = mtf.reshape(\n",
    "      x, mtf.Shape([batch_dim, num_w_blocks_dim, blocks_w_dim, model_dim]))\n",
    "  # [ self attention - ffn - residual + dropout] x n\n",
    "  mode = getattr(hparams, \"mode\", tf_estimator.ModeKeys.TRAIN)\n",
    "  is_training = mode == tf_estimator.ModeKeys.TRAIN\n",
    "  for layer in range(hparams.num_decoder_layers):\n",
    "    layer_name = \"decoder_layer_%d\" % layer\n",
    "    with tf.variable_scope(layer_name):\n",
    "      # Self attention layer\n",
    "      x += layer_prepostprocess_dropout(\n",
    "          mtf.layers.local_self_attention_spatial_blocks(\n",
    "              mtf.layers.layer_norm(x, model_dim, name=\"layer_norm_att\"),\n",
    "              kv_dim,\n",
    "              heads_dim,\n",
    "              is_training,\n",
    "              memory_w_dim=blocks_w_dim,\n",
    "              mask_right=True,\n",
    "              name=\"self_att\"), hparams)\n",
    "      # ffn layer\n",
    "      x += layer_prepostprocess_dropout(\n",
    "          mtf.layers.dense_relu_dense(\n",
    "              mtf.layers.layer_norm(x, model_dim, name=\"layer_norm_ffn\"),\n",
    "              feedforward_dim,\n",
    "              is_training,\n",
    "              hparams.dropout,\n",
    "              dropout_broadcast_dims=[length_dim]), hparams)\n",
    "\n",
    "  output = mtf.layers.layer_norm(x, model_dim, name=\"final_layer_norm\")\n",
    "  return output\n",
    "\n",
    "\n",
    "def local_attention2d_spatial_decoder(x, kv_dim, heads_dim,\n",
    "                                      feedforward_dim, hparams):\n",
    "  \"\"\"Image Transformer decoder with local2D spatial layers.\"\"\"\n",
    "  batch_dim, length_dim, model_dim = x.shape.dims\n",
    "  blocks_h_dim = mtf.Dimension(\"blocksh\", hparams.block_height)\n",
    "  blocks_w_dim = mtf.Dimension(\"blocksw\", hparams.block_width)\n",
    "  num_h_blocks_dim = mtf.Dimension(\"num_h_blocks\",\n",
    "                                   hparams.img_len // hparams.block_height)\n",
    "  num_w_blocks_dim = mtf.Dimension(\n",
    "      \"num_w_blocks\",\n",
    "      hparams.img_len * hparams.num_channels // hparams.block_width)\n",
    "  x = mtf.transpose(\n",
    "      mtf.reshape(\n",
    "          x,\n",
    "          mtf.Shape([\n",
    "              batch_dim, num_h_blocks_dim, blocks_h_dim,\n",
    "              num_w_blocks_dim, blocks_w_dim, model_dim\n",
    "          ])),\n",
    "      mtf.Shape([\n",
    "          batch_dim, num_h_blocks_dim, num_w_blocks_dim,\n",
    "          blocks_h_dim, blocks_w_dim, model_dim\n",
    "      ]))\n",
    "  mode = getattr(hparams, \"mode\", tf_estimator.ModeKeys.TRAIN)\n",
    "  is_training = mode == tf_estimator.ModeKeys.TRAIN\n",
    "  # Image Transformer Decoder\n",
    "  # [ self attention - ffn - residual + dropout] x n\n",
    "  for layer in range(hparams.num_decoder_layers):\n",
    "    layer_name = \"decoder_layer_%d\" % layer\n",
    "    with tf.variable_scope(layer_name):\n",
    "      # Self attention layer\n",
    "      x += layer_prepostprocess_dropout(\n",
    "          mtf.layers.local_2d_self_attention_spatial_blocks(\n",
    "              mtf.layers.layer_norm(x, model_dim, name=\"layer_norm_att\"),\n",
    "              kv_dim,\n",
    "              heads_dim,\n",
    "              is_training,\n",
    "              memory_h_dim=num_h_blocks_dim,\n",
    "              memory_w_dim=num_w_blocks_dim,\n",
    "              name=\"self_att\"), hparams)\n",
    "      # ffn layer\n",
    "      x += layer_prepostprocess_dropout(\n",
    "          mtf.layers.dense_relu_dense(\n",
    "              mtf.layers.layer_norm(x, model_dim, name=\"layer_norm_ffn\"),\n",
    "              feedforward_dim,\n",
    "              hparams.dropout,\n",
    "              dropout_broadcast_dims=[length_dim]), hparams)\n",
    "\n",
    "  output = mtf.layers.layer_norm(x, model_dim, name=\"final_layer_norm\")\n",
    "  return output\n",
    "\n",
    "\n",
    "def local_attention1d_masked_decoder(x, kv_dim, heads_dim,\n",
    "                                     feedforward_dim, hparams):\n",
    "  \"\"\"Image Transformer decoder with local1D masked layers.\"\"\"\n",
    "  print(x)\n",
    "  _, length_dim, model_dim = x.shape.dims\n",
    "  mode = getattr(hparams, \"mode\", tf_estimator.ModeKeys.TRAIN)\n",
    "  is_training = mode == tf_estimator.ModeKeys.TRAIN\n",
    "  for layer in range(hparams.num_decoder_layers):\n",
    "    layer_name = \"decoder_layer_%d\" % layer\n",
    "    with tf.variable_scope(layer_name):\n",
    "      # Self attention layer\n",
    "      length_per_split = mtf.tensor_dim_to_size_per_split(\n",
    "          hparams.layout, hparams.mesh_shape, length_dim)\n",
    "      x += layer_prepostprocess_dropout(\n",
    "          mtf.layers.masked_local_attention_1d(\n",
    "              mtf.layers.layer_norm(x, model_dim, name=\"layer_norm_att\"),\n",
    "              kv_dim,\n",
    "              heads_dim,\n",
    "              is_training,\n",
    "              window_size=hparams.block_length,\n",
    "              length_per_split=length_per_split,\n",
    "              name=\"self_att\"), hparams)\n",
    "      # ffn layer\n",
    "      x += layer_prepostprocess_dropout(\n",
    "          mtf.layers.dense_relu_dense(\n",
    "              mtf.layers.layer_norm(x, model_dim, name=\"layer_norm_ffn\"),\n",
    "              feedforward_dim,\n",
    "              hparams.dropout,\n",
    "              dropout_broadcast_dims=[length_dim]), hparams)\n",
    "\n",
    "  output = mtf.layers.layer_norm(x, model_dim, name=\"final_layer_norm\")\n",
    "  return output\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def mtf_image_transformer_base():\n",
    "  \"\"\"Set of hyperparameters.\"\"\"\n",
    "  hparams = common_hparams.basic_params1()\n",
    "  hparams.no_data_parallelism = True\n",
    "  hparams.use_fixed_batch_size = True\n",
    "  hparams.batch_size = 1\n",
    "  hparams.max_length = 3072\n",
    "  hparams.hidden_size = 256\n",
    "  hparams.label_smoothing = 0.0\n",
    "  # 8-way model-parallelism\n",
    "  hparams.add_hparam(\"mesh_shape\", \"batch:8\")\n",
    "  hparams.add_hparam(\"layout\", \"batch:batch\")\n",
    "  hparams.add_hparam(\"mtf_mode\", True)\n",
    "  hparams.add_hparam(\"num_heads\", 8)\n",
    "  hparams.add_hparam(\"filter_size\", 1024)\n",
    "  hparams.add_hparam(\"num_encoder_layers\", 0)\n",
    "  hparams.add_hparam(\"num_decoder_layers\", 6)\n",
    "  hparams.add_hparam(\"attention_key_size\", 256)\n",
    "  hparams.add_hparam(\"attention_value_size\", 256)\n",
    "  # Share weights between input and target embeddings\n",
    "  hparams.shared_embedding = True\n",
    "\n",
    "  # mixture of experts hparams\n",
    "  hparams.add_hparam(\"ffn_layer\", \"dense_relu_dense\")\n",
    "  hparams.add_hparam(\"moe_overhead_train\", 1.0)\n",
    "  hparams.add_hparam(\"moe_overhead_eval\", 2.0)\n",
    "  hparams.moe_num_experts = 16\n",
    "  hparams.moe_loss_coef = 1e-3\n",
    "\n",
    "  hparams.shared_embedding_and_softmax_weights = True\n",
    "  hparams.optimizer = \"Adafactor\"\n",
    "  hparams.learning_rate_schedule = \"rsqrt_decay\"\n",
    "  hparams.learning_rate_warmup_steps = 10000\n",
    "  hparams.add_hparam(\"d_kv\", 64)\n",
    "  hparams.add_hparam(\"d_ff\", 2048)\n",
    "\n",
    "  # Image related hparams\n",
    "  hparams.add_hparam(\"img_len\", 32)\n",
    "  hparams.add_hparam(\"num_channels\", 3)\n",
    "  hparams.add_hparam(\"unconditional\", True)\n",
    "\n",
    "  # Local Attention related params\n",
    "  hparams.add_hparam(\"block_length\", 128)\n",
    "  hparams.add_hparam(\"block_height\", 16)\n",
    "  hparams.add_hparam(\"block_width\", 16)\n",
    "  hparams.add_hparam(\"attention_type\", \"local1d\")\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def mtf_image_transformer_tiny():\n",
    "  \"\"\"Catch bugs locally...\"\"\"\n",
    "  hparams = mtf_image_transformer_base()\n",
    "  hparams.hidden_size = 128\n",
    "  hparams.d_ff = 256\n",
    "  hparams.batch_size = 4\n",
    "  hparams.num_encoder_layers = 1\n",
    "  hparams.num_decoder_layers = 4\n",
    "  hparams.num_heads = 4\n",
    "  hparams.attention_key_size = 128\n",
    "  hparams.attention_value_size = 128\n",
    "  hparams.block_length = 32\n",
    "  # data parallelism and model-parallelism\n",
    "  hparams.mesh_shape = \"batch:2\"\n",
    "  hparams.layout = \"batch:batch\"\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def mtf_image_transformer_single():\n",
    "  \"\"\"Small single parameters.\"\"\"\n",
    "  hparams = mtf_image_transformer_tiny()\n",
    "  hparams.mesh_shape = \"\"\n",
    "  hparams.layout = \"\"\n",
    "  hparams.hidden_size = 32\n",
    "  hparams.filter_size = 32\n",
    "  hparams.batch_size = 1\n",
    "  hparams.num_encoder_layers = 1\n",
    "  hparams.num_decoder_layers = 1\n",
    "  hparams.num_heads = 2\n",
    "  hparams.attention_key_size = 32\n",
    "  hparams.attention_value_size = 32\n",
    "  hparams.block_length = 16\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def mtf_image_transformer_base_single():\n",
    "  \"\"\"Small single parameters.\"\"\"\n",
    "  hparams = mtf_image_transformer_base()\n",
    "  hparams.num_decoder_layers = 6\n",
    "  hparams.filter_size = 256\n",
    "  hparams.block_length = 128\n",
    "  hparams.mesh_shape = \"\"\n",
    "  hparams.layout = \"\"\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def mtf_image_transformer_tiny_spatial1d():\n",
    "  \"\"\"Small single parameters.\"\"\"\n",
    "  hparams = mtf_image_transformer_tiny()\n",
    "  hparams.num_decoder_layers = 6\n",
    "  hparams.filter_size = 128\n",
    "  hparams.block_height = 8\n",
    "  hparams.block_width = 8\n",
    "  hparams.attention_type = \"local1d_spatial\"\n",
    "  hparams.mesh_shape = \"\"\n",
    "  hparams.layout = \"\"\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def mtf_image_transformer_tiny_spatial2d():\n",
    "  \"\"\"Small single parameters.\"\"\"\n",
    "  hparams = mtf_image_transformer_tiny()\n",
    "  hparams.num_decoder_layers = 6\n",
    "  hparams.filter_size = 128\n",
    "  hparams.block_height = 8\n",
    "  hparams.block_width = 8\n",
    "  hparams.attention_type = \"local2d_spatial\"\n",
    "  hparams.mesh_shape = \"b1:2,b2:2\"\n",
    "  hparams.layout = \"num_h_blocks:b1,num_wblocks:b2\"\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def mtf_image_transformer_base_cifar():\n",
    "  \"\"\"Data parallel CIFAR parameters.\"\"\"\n",
    "  hparams = mtf_image_transformer_base()\n",
    "  hparams.mesh_shape = \"batch:8\"\n",
    "  hparams.layout = \"batch:batch\"\n",
    "  hparams.learning_rate_decay_steps = 13600  # one epoch\n",
    "  hparams.batch_size = 32\n",
    "  hparams.num_heads = 4\n",
    "  hparams.num_decoder_layers = 12\n",
    "  hparams.block_length = 256\n",
    "  hparams.hidden_size = 512\n",
    "  hparams.d_ff = 2048\n",
    "  hparams.learning_rate = 0.5\n",
    "  hparams.layer_preprocess_sequence = \"none\"\n",
    "  hparams.layer_postprocess_sequence = \"dan\"\n",
    "  hparams.layer_prepostprocess_dropout = 0.3\n",
    "  hparams.unconditional = True\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def mtf_image_transformer_cifar_4x():\n",
    "  \"\"\"Data parallel CIFAR parameters.\"\"\"\n",
    "  hparams = mtf_image_transformer_base_cifar()\n",
    "  hparams.mesh_shape = \"batch:32\"\n",
    "  hparams.layout = \"batch:batch\"\n",
    "  hparams.batch_size = 128\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def mtf_image_transformer_cifar_mp_4x():\n",
    "  \"\"\"Data parallel CIFAR parameters.\"\"\"\n",
    "  hparams = mtf_image_transformer_base_cifar()\n",
    "  hparams.mesh_shape = \"model:4;batch:8\"\n",
    "  hparams.layout = \"batch:batch;d_ff:model;heads:model\"\n",
    "  hparams.batch_size = 32\n",
    "  hparams.num_heads = 8\n",
    "  hparams.d_ff = 8192\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def mtf_image_transformer_base_imagenet():\n",
    "  \"\"\"Data parallel CIFAR parameters.\"\"\"\n",
    "  hparams = mtf_image_transformer_base_cifar()\n",
    "  hparams.mesh_shape = \"batch:32\"\n",
    "  hparams.layout = \"batch:batch\"\n",
    "  hparams.batch_size = 128\n",
    "  hparams.d_ff = 2048\n",
    "  hparams.hidden_size = 512\n",
    "  hparams.num_decoder_layers = 12\n",
    "  hparams.learning_rate = 0.5\n",
    "  hparams.learning_rate_warmup_steps = 31250\n",
    "  hparams.layer_preprocess_sequence = \"none\"\n",
    "  hparams.layer_postprocess_sequence = \"dan\"\n",
    "  hparams.layer_prepostprocess_dropout = 0.1\n",
    "  hparams.unconditional = True\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def mtf_image_transformer_base_imagenet_mp():\n",
    "  \"\"\"Model parallel ImageNet parameters.\"\"\"\n",
    "  hparams = mtf_image_transformer_base_imagenet()\n",
    "  hparams.mesh_shape = \"model:4;batch:8\"\n",
    "  hparams.layout = \"batch:batch;d_ff:model;heads:model\"\n",
    "  hparams.batch_size = 32\n",
    "  hparams.num_heads = 8\n",
    "  hparams.d_ff = 8192\n",
    "  hparams.learning_rate_warmup_steps = 31250\n",
    "  hparams.unconditional = True\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def mtf_image_transformer_base_imagenet_mp128():\n",
    "  \"\"\"Model parallel ImageNet parameters.\"\"\"\n",
    "  hparams = mtf_image_transformer_base_imagenet()\n",
    "  hparams.mesh_shape = \"model:8;batch:4\"\n",
    "  hparams.layout = \"batch:batch;d_ff:model;heads:model\"\n",
    "  hparams.batch_size = 8\n",
    "  hparams.img_len = 128\n",
    "  hparams.block_length = 128\n",
    "  hparams.num_heads = 8\n",
    "  hparams.num_decoder_layers = 4\n",
    "  hparams.d_ff = 4096\n",
    "  hparams.learning_rate_warmup_steps = 31250\n",
    "  hparams.unconditional = True\n",
    "  hparams.max_length = 256*256*3\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def mtf_image_transformer_base_imagenet_mp_sp():\n",
    "  \"\"\"Model parallel ImageNet parameters.\"\"\"\n",
    "  hparams = mtf_image_transformer_base_imagenet_mp128()\n",
    "  hparams.mesh_shape = \"model:8;batch:4\"\n",
    "  hparams.layout = \"batch:batch;d_ff:model;num_wblocks:model\"\n",
    "  hparams.batch_size = 8\n",
    "  hparams.img_len = 128\n",
    "  hparams.block_length = 128\n",
    "  hparams.attention_type = \"local1d_spatial\"\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def mtf_image_transformer_base_imagenet_mp64():\n",
    "  \"\"\"Model parallel ImageNet parameters.\"\"\"\n",
    "  hparams = mtf_image_transformer_base_imagenet()\n",
    "  hparams.mesh_shape = \"model:8;batch:4\"\n",
    "  hparams.layout = \"batch:batch;d_ff:model;heads:model\"\n",
    "  hparams.batch_size = 8\n",
    "  hparams.img_len = 64\n",
    "  hparams.num_decoder_layers = 8\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def mtf_image_transformer_tiny_8gpu():\n",
    "  hparams = mtf_image_transformer_tiny()\n",
    "  hparams.mesh_shape = \"all:8\"\n",
    "  hparams.layout = \"vocab:all;filter_size:all;heads:all\"\n",
    "  return hparams\n",
    "\n",
    "\n",
    "@registry.register_hparams\n",
    "def mtf_image_transformer_length_sharded():\n",
    "  hparams = mtf_image_transformer_tiny()\n",
    "  hparams.mesh_shape = \"all:2\"\n",
    "  hparams.layout = \"length:all\"\n",
    "  return hparams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
